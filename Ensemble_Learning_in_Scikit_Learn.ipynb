{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble Learning in Scikit-Learn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPsCHUGQ8fkUGBUUkham/r1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruizleandro/Ensemble-Learning-and-Boosting/blob/main/Ensemble_Learning_in_Scikit_Learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsMkIju5RnrC"
      },
      "source": [
        "Source: Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow (Chapter 7)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V_21oqwScfp"
      },
      "source": [
        "# Quick Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRBfqnsyMAek"
      },
      "source": [
        "> Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble's accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qImkNjLfRh76",
        "outputId": "dded054e-03a4-4abe-adb3-9039870e5668"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier()\n",
        "svm_clf = SVC()\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='hard')\n",
        "\n",
        "X, y = make_moons(n_samples=1000, noise=0.3, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.35,\n",
        "                                                    random_state=42)\n",
        "\n",
        "voting_clf.fit(X_train, y_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('lr',\n",
              "                              LogisticRegression(C=1.0, class_weight=None,\n",
              "                                                 dual=False, fit_intercept=True,\n",
              "                                                 intercept_scaling=1,\n",
              "                                                 l1_ratio=None, max_iter=100,\n",
              "                                                 multi_class='auto',\n",
              "                                                 n_jobs=None, penalty='l2',\n",
              "                                                 random_state=None,\n",
              "                                                 solver='lbfgs', tol=0.0001,\n",
              "                                                 verbose=0, warm_start=False)),\n",
              "                             ('rf',\n",
              "                              RandomForestClassifier(bootstrap=True,\n",
              "                                                     ccp_alpha=0.0,\n",
              "                                                     class_weight=None,\n",
              "                                                     cr...\n",
              "                                                     oob_score=False,\n",
              "                                                     random_state=None,\n",
              "                                                     verbose=0,\n",
              "                                                     warm_start=False)),\n",
              "                             ('svc',\n",
              "                              SVC(C=1.0, break_ties=False, cache_size=200,\n",
              "                                  class_weight=None, coef0=0.0,\n",
              "                                  decision_function_shape='ovr', degree=3,\n",
              "                                  gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                                  probability=False, random_state=None,\n",
              "                                  shrinking=True, tol=0.001, verbose=False))],\n",
              "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
              "                 weights=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nw9BL1hUc-0",
        "outputId": "ac1d89c2-6a48-450c-91ac-85e61ce92d0e"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression 0.8514285714285714\n",
            "RandomForestClassifier 0.9142857142857143\n",
            "SVC 0.9028571428571428\n",
            "VotingClassifier 0.9057142857142857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRvI2PndWAZZ"
      },
      "source": [
        "As you can see, the voting classifier slightly outperforms all the individual classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmNIDnhwLjY8"
      },
      "source": [
        "If all classifiers are able to estimate class probabilities (i.e., they have a `predict_proba()` method), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called _soft voting_. It often achieves higher performance than ahrd voting because it gives more weight to highly confident votes. All you need to do is replace `voting='hard'` with `voting='soft'` and ensure that all classifiers can estimate class probabilities. This is not the case of the `SVC` class by default, so you need to set its `probability` hyperparameter to `True` (this will make the `SVC` class use cross-validation to estimate class probabilities, slowing down training, and it will add a `predict_proba()` method). Let's see how it looks our ensemble model with these modifications:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN0yoZxEMzFq",
        "outputId": "494d2f57-7c63-4c46-f4b9-17ac2917cd20"
      },
      "source": [
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier()\n",
        "svm_clf = SVC(probability=True)\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='soft')\n",
        "\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression 0.8514285714285714\n",
            "RandomForestClassifier 0.92\n",
            "SVC 0.9028571428571428\n",
            "VotingClassifier 0.9142857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wz_0451WPLp"
      },
      "source": [
        "# Bagging and Pasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_FUl1hCWcOR"
      },
      "source": [
        "One way to get a diverse set of classifiers is to use very different training algorithms, as just discussed. Another approach is to use the same training algorithm for every predictor, but to train them on different random subsets of the training set. When sampling is performed _with_ replacement, this method is called _bagging_ (short for _bootstrap aggregating_). When sampling is performed _without_ replacement, it is called _pasting_.\n",
        "\n",
        "In other words, both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor.\n",
        "\n",
        "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is typically the _statistical mode_ (i.e., the most frequent prediction, just like a hard voting classifier) for classification, or the average for regression. Each individual predictor has a higher bias than if it were trained on the original training set, but aggregation reduces both bias and variance. Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsNpnem1YLrK"
      },
      "source": [
        "## Bagging and Pasting in Scikit-Learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpnFej54ZxkA"
      },
      "source": [
        "You can use `BaggingClassifier` for classification tasks, and `BaggingRegressor` for regression.\n",
        "\n",
        "Some parameters for the `BaggingClassifier`:\n",
        "\n",
        "* `n_estimators`: number of classifiers used in the ensemble.\n",
        "* `max_samples`: number of samples from the training set to fit every classifier.\n",
        "* `bootstrap`: `True` for using bagging, `False` for using pasting.\n",
        "* `n_jobs`: number of CPU cores to use for training (`-1` means use all available cores).\n",
        "\n",
        "Here's an example of bagging:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Pmm3RyUYSog",
        "outputId": "e3ca4c3d-0e18-4044-b3c6-820ab3d1386a"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), n_estimators=1000,\n",
        "    max_samples=200, bootstrap=True, n_jobs=-1, random_state=0)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9057142857142857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-479hIKabchv"
      },
      "source": [
        "And here an example of pasting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gz2F1MiaaJx",
        "outputId": "5bd56f27-3879-46c3-99d8-8adb3e1c827a"
      },
      "source": [
        "pas_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), n_estimators=1000,\n",
        "    max_samples=200, bootstrap=False, n_jobs=-1, random_state=0)\n",
        "pas_clf.fit(X_train, y_train)\n",
        "y_pred = pas_clf.predict(X_test)\n",
        "\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9085714285714286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoRihOPoZsio"
      },
      "source": [
        "Here is the example with only one tree doing all the job:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm_kK8x0ZOEh",
        "outputId": "0ccd879d-3e4a-4981-9fe0-6140136e6016"
      },
      "source": [
        "tree = DecisionTreeClassifier()\n",
        "tree.fit(X_train, y_train)\n",
        "y_pred = tree.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8714285714285714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81JbDQXhcCpb"
      },
      "source": [
        "## Out-of-Bag Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7G4Y8o6cFbA"
      },
      "source": [
        "With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. By default a `BaggingClassifier` samples _m_ training instances with replacement (`bootstrap=True`), where _m_ is the size of the trainin set. This means that only about 63 % of the training instances are sampled on average for each predictor. The remaining 37 % of the training instances that are not sampled are called _out-of-bag_ (oob) instances. Note that they are not the same 37 % for all predictors. \n",
        "\n",
        "Since a predictor never sees the oob instances during training, it can be evaluated on these instances without the need for a separate validation set. You can evaluate the ensemble itself by averaging out the oob evaluations of each predictor. \n",
        "\n",
        "In Scikit-Learn, you can set `oob_score=True` when creating a `BaggingClassifier` to request an automatic oob evaluation after training. The resulting evaluation score is available through the `oob_score_` variable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tatguu8dGHQ",
        "outputId": "39e8d897-e6bb-4ceb-b252-dd24c924cbac"
      },
      "source": [
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), n_estimators=500,\n",
        "    bootstrap=True, n_jobs=-1, oob_score=True)\n",
        "\n",
        "bag_clf.fit(X_train, y_train)\n",
        "bag_clf.oob_score_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9061538461538462"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj7e0FGXdghF"
      },
      "source": [
        "According to this oob evaluation, this `BaggingClassifier` is likely to achieve about 90.6 % accuracy on the test set. Let's verify it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h83oxTMRdq5u",
        "outputId": "9cd9e875-d17c-4b17-eae7-cdac2c6c2924"
      },
      "source": [
        "y_pred = bag_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9142857142857143"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz5trJ18dutx"
      },
      "source": [
        "We get 91.4 % accuracy on the test set! Close enough!\n",
        "\n",
        "Even we have available the oob decision function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6LY32rYeiyH",
        "outputId": "ad7a138a-31f5-41a2-8a10-26b2f04fba9f"
      },
      "source": [
        "bag_clf.oob_decision_function_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 1.        ],\n",
              "       [0.97206704, 0.02793296],\n",
              "       [1.        , 0.        ],\n",
              "       ...,\n",
              "       [0.99408284, 0.00591716],\n",
              "       [0.88333333, 0.11666667],\n",
              "       [0.        , 1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVJM4fFCBCcC"
      },
      "source": [
        "## Random Patches and Random Subspaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOajq17_BFAa"
      },
      "source": [
        "The `BaggingClassifier` class supports sampling the features as well. This is controlled by two hyperparameters: `max_features` and `bootstrap_features`. They work the same way as `max_samples` and `bootstrap`, but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features.\n",
        "\n",
        "This is particularly useful when you are dealing with high-dimensional inputs (such as images). Sampling both training instances and features is called _Random Patches_ method. Keeping all training instances (i.e., `bootstrap=False` and `max_samples=1.0`) but sampling features (i.e., `bootstrap_features=True` and/or `max_features` smaller than 1.0) is called the _Random Subspaces_ method.\n",
        "\n",
        "Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwcUfT8oC2Iy"
      },
      "source": [
        "# Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvC_XMzSC3C_"
      },
      "source": [
        "_Boosting_ (originally called _hypothesis boosting_) refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. There are many boosting methods available, but by far the most popular are _AdaBoost_ (short for _Adaptive Boosting_) and _Gradient Boosting_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ3LalZ5DThc"
      },
      "source": [
        "## AdaBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8Cd5WSBDY0P"
      },
      "source": [
        "One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases. This is the technique used by AdaBoost.\n",
        "\n",
        "For example, to build an AdaBoost classifier, a first base classifier (such as a Decision Tree) is trained and used to make predictions on the training set. The relative weight of misclassified training instances is then increased. A second classifier is trained using the updated weights and again it makes predictions on the training set, weights are updated, and so on.\n",
        "\n",
        "Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, except that the predictors have different weights depending on their overall accuracy on the weighted training set.\n",
        "\n",
        "Scikit-Learn actually uses a multiclass versoin of AdaBoost called SAMME (which stands for _Stagewise Additive Modeling using a Multiclass Exponential loss function_). When there are just two classes, SAMME is equivalent to AdaBoost. Moreover, if the predictors can estimate class probabilities (i.e., if they have a `predict_proba()` method), Scikit-Learn can use a variant of SAMME called SAMME.R (the _R_ stands for \"Real\"), which relies on class probabilities rather than predictions and generally performs better.\n",
        "\n",
        "The following code trains an AdaBoost classifier based on 200 _Decision Stumps_ using Scikit-Learn's `AdaBoostClassifier` class (as you mgiht expect, there is also an `AdaBoostRegressor` class). A Decision Stump is a Decision Tree with `max_depth=1`--i.e., a tree composed of a single decision node plus two leaf nodes. This is the default base estimator for the `AdaBoostClassifier` class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qA_nwD4Fyd8",
        "outputId": "d1a7e6f3-fe9c-43f1-fd1a-7fbf9f2ac9e8"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
        "    algorithm='SAMME.R', learning_rate=0.5)\n",
        "ada_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(algorithm='SAMME.R',\n",
              "                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
              "                                                         class_weight=None,\n",
              "                                                         criterion='gini',\n",
              "                                                         max_depth=1,\n",
              "                                                         max_features=None,\n",
              "                                                         max_leaf_nodes=None,\n",
              "                                                         min_impurity_decrease=0.0,\n",
              "                                                         min_impurity_split=None,\n",
              "                                                         min_samples_leaf=1,\n",
              "                                                         min_samples_split=2,\n",
              "                                                         min_weight_fraction_leaf=0.0,\n",
              "                                                         presort='deprecated',\n",
              "                                                         random_state=None,\n",
              "                                                         splitter='best'),\n",
              "                   learning_rate=0.5, n_estimators=200, random_state=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm8ioHv5GNcC",
        "outputId": "d0429607-7a56-4087-a1aa-dc95e50759cd"
      },
      "source": [
        "y_pred = ada_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9085714285714286"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDCR341KGDRa"
      },
      "source": [
        "> If your AdaBoost ensemble is overfitting the training set, you can try reducing the number of estimators of more strongly regularizing the base estimator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abl2ysNqGY95"
      },
      "source": [
        "## Gradient Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S7RSRyJGanJ"
      },
      "source": [
        "Another very popular Boosting algorithm is _Gradient Boosting_. Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, eahc one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the _residual errors_ made by the previous predictor.\n",
        "\n",
        "> Note: the following code and text are an extract from the notebook \"_Supervised Learning_\":\n",
        "\n",
        "The gradient boosted regression tree is another ensemble method that combines multiple decision trees to create more powerful model. Despite the \"regression\" in the name, these models can be used for regression and classification. In contrast to the random forest approach, gradient boosting works by building trees in a serial manner, where each tree tries to correct the mistakes of the previous one. By default, there is no randomization in gradient boosted trees; instead, strong pre-pruning is used. Gradient boosted trees often use very shallow trees, of depth one to five, which makes the model smaller in terms of memory and makes prediction faster.\n",
        "\n",
        "The main idea behind gradient boosting is to combine many simple models (in this context known as weak learners), like shallow trees. Each tree can only provide good predictions on part of the data, and so more and more trees are added to iteratively improve performance.\n",
        "\n",
        "Gradient boosted trees are frequently the winning entries in machine learning competitions, and are widely used in industry. They are generally a bit more sensitive to parameter settings than random forests, but can provide better accuracy if the parametersare set correctly.\n",
        "\n",
        "Apart for the pre-pruning and the number of trees in the ensemble, another important parameter of gradient boosting is the learning_rate which controls how strongly each tree can make stronger corrections, allowing for more complex models. Adding more trees to the ensemble, which can be accomplished by increasing n_estimators, also increases the model complexity, as the model has more chances to correct mistakes on the training set.\n",
        "\n",
        "Here is an example of using GradientBoostingClassifier on the Breast Cancer dataset. By default, 100 trees of maximum depth 3 and a learning rate of 0.1 are used:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qka468xeKYOR",
        "outputId": "b3cb76d2-504f-4551-e0c0-7cd27472ed34"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
        "\n",
        "gbrt = GradientBoostingClassifier(random_state=0)\n",
        "gbrt.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
        "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on training set: 1.000\n",
            "Accuracy on test set: 0.965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjzcb_D3KdVe"
      },
      "source": [
        "As the training set accuracy is 100 %, we are likely to be overfitting. To reduce overfitting, we could either apply stronger pre-pruning by limiting the maximum depth or lower the learning rate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB6Ri9n5uHu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7b0245d-84a2-43cb-dcf9-c7791a52fe07"
      },
      "source": [
        "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
        "gbrt.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
        "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on training set: 0.991\n",
            "Accuracy on test set: 0.972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjXhgG38uXJn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b387914d-f164-4dd4-c3dc-41a63af0dfa9"
      },
      "source": [
        "gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\n",
        "gbrt.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
        "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on training set: 0.988\n",
            "Accuracy on test set: 0.965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYHpaU4HucMi"
      },
      "source": [
        "Both methods of decreasing the model complexity reduced the training set accuracy, as expected. In this case, lowering the maximum depth of the trees provided a significant improvement of the model, while lowering the learning rate only increased the generalization performance slightly.\n",
        "\n",
        "As for the other decision tree-based models, we can again visualize the feature importances to get more insight into our model. As we used 100 trees, it is impractical to inspect them all, even if they are all of depth 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SzVxjWMvFx3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "3117f905-8a1d-4bf2-936f-b45b627aa405"
      },
      "source": [
        "def plot_feature_importances_cancer(model):\n",
        "  n_features = cancer.data.shape[1]\n",
        "  plt.barh(range(n_features), model.feature_importances_, align='center')\n",
        "  plt.yticks(np.arange(n_features), cancer.feature_names)\n",
        "  plt.xlabel(\"Feature importance\")\n",
        "  plt.ylabel(\"Feature\")\n",
        "\n",
        "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
        "gbrt.fit(X_train, y_train)\n",
        "\n",
        "plot_feature_importances_cancer(gbrt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAegAAAEGCAYAAACuHgb+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue0hKhtbw4Hq"
      },
      "source": [
        "We can see that the feature importances of the gradient boosted trees are somewhat similar to hte feature importances of the random forests, though the gradient boosting completely ignored some of the features.\n",
        "\n",
        "As both gradient boosting and random forests perform well on similar kinds of data, a common approach os to first try random forests, which work quite robustly. If random forests work well but prediction time is at a premium, or it is important to squeeze out the last percentage of accuracy from the machine learning model, moving to gradient boosting often helps. \n",
        "\n",
        "If you want to apply gradient boosting to a large-scale problem, it might be worth looking into the `xgboost` package and its Python interface, which at the time of writing is faster (and sometimes easier to tune) than the `scikit-learn` implementation of gradient boosting on many datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3medAOJ81rL"
      },
      "source": [
        "**Strenghts, weaknesses, and parameters.** Gradient boosted decision trees are among the most powerful and widely used models for supervised learning. Their main drawback is that they require careful tuning of the parameters and may take a long time to train. Similarly to other tree-based models, the algorithm works well without scaling and on mixture of binary and continuous features. As with other tree-based models, it also often does not work well on high-dimensional sparse data.\n",
        "\n",
        "The main parameters of gradient boosted tree models are the number of trees, `n_estimators`, and the `learning_rate`, which controls the degree to which each tree  is allowed to correct the mistakes of the previous trees. These two parameters are highly interconnected, as a lower `learning_rate` means that more trees are needed to build a model of similar complexity. In contrast to random forests, where a higher `n_estimators` value is always better, increasing `n_estimators` depending on the time and memory budget, and then search over different `learning_rate`s.\n",
        "\n",
        "Another important parameter is `max_depth` (or alternatively `max_leaf_nodes`), to reduce the complexity of each tree. Usually `max_depth` is set very low for gradient boosted models, often not deeper than five splits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XYJ6nEyNWvi"
      },
      "source": [
        "# Stacking"
      ]
    }
  ]
}